{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Градиентный спуск и его модификации\n",
    "   - Выбрать [тестовые функции оптимизации](https://ru.wikipedia.org/wiki/Тестовые_функции_для_оптимизации) (2 шт)\n",
    "   - Запрограммировать собственную реализацию классического градиентного спуска\n",
    "   - Запрограммировать пайлайн тестирования алгоритма оптимизации\n",
    "     - Визуализации функции и точки оптимума\n",
    "     - Вычисление погрешности найденного решения в сравнение с аналитическим для нескольких запусков\n",
    "     - Визуализации точки найденного решения (можно добавить анимацию на плюс балл)\n",
    "   - Запрограммировать метод вычисления градиента\n",
    "     - Передача функции градиента от пользователя\n",
    "     - Символьное вычисление градиента (например с помощью [sympy](https://www.sympy.org/en/index.html)) (на доп балл)\n",
    "     - Численная аппроксимация градиента (на доп балл)\n",
    "   - Запрограммировать одну моментную модификацию и протестировать ее\n",
    "   - Запрограммировать одну адаптивную модификацию и протестировать ее\n",
    "   - Запрограммировать метод эволюции темпа обучения и/или метод выбора начального приближения и протестировать \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм градиентного спуска и модификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import sympy as sp\n",
    "\n",
    "\n",
    "def gradient_descent(grad_func, x_init, learning_rate=0.001, max_iter=10000, tol=1e-6):\n",
    "    x = x_init\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_func(x)\n",
    "        x_next = x - learning_rate * grad\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            print(f\"Converged in {i} iterations\")\n",
    "            break\n",
    "        x = x_next\n",
    "    return x\n",
    "\n",
    "\n",
    "def learn_nesterov(grad_func, x_init, momentum=0.9, learning_rate=0.0001, max_iter=10000, , tol=1e-6):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def Adagrad():\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_optimization(func, grad_func, x_init, optimizer, **kwargs):\n",
    "    sp_x_res = minimize(func, x_init).x\n",
    "    optimal_x = optimizer(func, grad_func, x_init, **kwargs)\n",
    "    return optimal_x, func(optimal_x), sp_x_res, func(sp_x_res)\n",
    "\n",
    "\n",
    "def plot_function_and_optimum(func, x_range, y_range, optimal_x):\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = np.array([func(np.array([x, y])) for x, y in zip(X.flatten(), Y.flatten())]).reshape(X.shape)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cp = plt.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "    plt.colorbar(cp)\n",
    "    plt.plot(optimal_x[0], optimal_x[1], 'ro')\n",
    "    plt.title('Function Contour and Optimum Point')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестовые функции и градиенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y):\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "\n",
    "def rosenbrock_grad(x, y):\n",
    "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dy = 200 * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "def himmelblau(x, y):\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "\n",
    "def himmelblau_grad(x, y):\n",
    "    dx = 4*x*(x**2 + y - 11) + 2*(x + y**2 - 7)\n",
    "    dy = 2*(x**2 + y - 11) + 4*y*(x + y**2 - 7)\n",
    "    return np.array([dx, dy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_init = np.random.randn(2)\n",
    "# optimal_x = gradient_descent(rosenbrock, grad_rosenbrock, x_init)      \n",
    "\n",
    "# print(\"Optimal x:\", optimal_x)\n",
    "# print(\"Function value at optimal x:\", rosenbrock(optimal_x))\n",
    "\n",
    "\n",
    "# results = test_optimization(rosenbrock, grad_rosenbrock, x_init, gradient_descent, learning_rate=0.001, max_iter=10000)\n",
    "# print(\"Test results:\", results)\n",
    "\n",
    "\n",
    "# x_range = np.linspace(-4, 4, 400)\n",
    "# y_range = np.linspace(-4, 4, 400)\n",
    "# plot_function_and_optimum(rosenbrock, x_range, y_range, optimal_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
